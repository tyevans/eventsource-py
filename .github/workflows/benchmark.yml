# Benchmark Workflow
# Runs performance benchmarks on push to main and pull requests
#
# Jobs:
# - benchmark: Run pytest-benchmark tests and compare results

name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: pyproject.toml

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run benchmarks
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=min,max,mean,stddev,median,ops \
            --benchmark-sort=name \
            -v

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 30

      # Store benchmark result for continuous tracking
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          benchmark-data-dir-path: 'dev/bench'
          # Skip fetching gh-pages if it doesn't exist (first run)
          skip-fetch-gh-pages: true

      # Comment on PR with benchmark comparison
      - name: Comment benchmark results on PR
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'pull_request'
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          comment-always: true
          alert-threshold: '150%'
          fail-on-alert: false
          benchmark-data-dir-path: 'dev/bench'
          # Skip fetching gh-pages if it doesn't exist (first run)
          skip-fetch-gh-pages: true
